{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEAsuH+8Qe7zvJCe9BhFtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GIBSONGODSAN/MachineLearningAlgorithms/blob/main/ArtificialNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network\n",
        "The Artificial Neural Network (ANN) is a machine learning algorithm inspired by the structure and function of the human brain. ANN can be trained to perform classification, regression, and other tasks. The algorithm works as follows:\n",
        "\n",
        "  Define the architecture: Decide on the number of layers and nodes in each layer of the neural network. The input layer receives input data, the output layer produces the output, and the hidden layers perform intermediate computations.\n",
        "\n",
        "  Initialize the weights: Randomly initialize the weights between the nodes of each layer.\n",
        "\n",
        "  Feedforward: Pass the input data through the network from the input layer to the output layer. In each layer, compute the weighted sum of the inputs and apply a non-linear activation function such as sigmoid, ReLU, or tanh.\n",
        "\n",
        "  Calculate error: Calculate the difference between the predicted output and the actual output using a loss function such as mean squared error or cross-entropy.\n",
        "\n",
        "  Backpropagation: Propagate the error backwards through the network, adjusting the weights to minimize the error using an optimization algorithm such as stochastic gradient descent. Update the weights in each layer by computing the gradient of the loss function with respect to the weights.\n",
        "\n",
        "  Repeat steps 3 to 5: Repeat the feedforward and backpropagation steps for a given number of epochs or until the error converges.\n",
        "\n",
        "  Output the predicted result: Once the neural network has been trained, use it to make predictions on new input data by passing it through the network and obtaining the output.\n",
        "\n",
        "ANN is a powerful algorithm that can learn complex non-linear relationships between inputs and outputs. However, it can be computationally expensive and requires careful tuning of the architecture, activation functions, and optimization algorithm. The choice of loss function is also important and depends on the task at hand. ANN is widely used in many fields, including computer vision, natural language processing, and speech recognition."
      ],
      "metadata": {
        "id": "h039svq7dm5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X / np.amax(X, axis=0)  # maximum of X array longitudinally\n",
        "y = y / 100\n",
        "\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "# Variable initialization\n",
        "epoch = 7000  # Setting training iterations\n",
        "lr = 0.1  # Setting learning rate\n",
        "inputlayer_neurons = 2  # number of features in data set\n",
        "hiddenlayer_neurons = 3  # number of hidden layers neurons\n",
        "output_neurons = 1  # number of neurons at output layer\n",
        "# weight and bias initialization\n",
        "wh = np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))\n",
        "bh = np.random.uniform(size=(1, hiddenlayer_neurons))\n",
        "wout = np.random.uniform(size=(hiddenlayer_neurons, output_neurons))\n",
        "bout = np.random.uniform(size=(1, output_neurons))\n",
        "# draws a random range of numbers uniformly of dim x*y\n",
        "for i in range(epoch):\n",
        "    # Forward Propogation\n",
        "    hinp1 = np.dot(X, wh)\n",
        "    hinp = hinp1 + bh\n",
        "    hlayer_act = sigmoid(hinp)\n",
        "    outinp1 = np.dot(hlayer_act, wout)\n",
        "    outinp = outinp1 + bout\n",
        "    output = sigmoid(outinp)\n",
        "    # Backpropagation\n",
        "    EO = y - output\n",
        "    outgrad = derivatives_sigmoid(output)\n",
        "    d_output = EO * outgrad\n",
        "    EH = d_output.dot(wout.T)\n",
        "    hiddengrad = derivatives_sigmoid(hlayer_act)\n",
        "    d_hiddenlayer = EH * hiddengrad\n",
        "    wout += hlayer_act.T.dot(d_output) * lr\n",
        "    # bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
        "    wh += X.T.dot(d_hiddenlayer) * lr\n",
        "    # bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
        "print(\"Input: \\n\" + str(X))\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "print(\"Predicted Output: \\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J7ROTdqdqZT",
        "outputId": "cd6f3bfb-1c85-4e3e-f90f-925d9e054294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "Predicted Output: \n",
            " [[0.89747924]\n",
            " [0.8783619 ]\n",
            " [0.8937405 ]]\n"
          ]
        }
      ]
    }
  ]
}